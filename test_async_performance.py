#!/usr/bin/env python3
"""
ØªØ³Øª performance Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Sync vs Async
"""

import sys
import time
import asyncio
import logging
import psutil
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src
sys.path.append(str(Path(__file__).parent / "src"))

from src.database.sqlite_manager import SQLiteManager
from src.feature_engineering.extractors import BankingFeatureExtractor
from src.feature_engineering.async_extractors import AsyncBankingFeatureExtractor
from src.analysis.clustering import BankingCustomerClustering  
from src.analysis.async_clustering import AsyncBankingCustomerClustering
from src.analysis.anomaly_detection import BankingAnomalyDetector
from src.analysis.async_anomaly_detection import AsyncBankingAnomalyDetection

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AsyncPerformanceTester:
    """ØªØ³Øª performance async vs sync"""
    
    def __init__(self, db_path: str = "database/banking_data.db"):
        self.db_path = db_path
        self.results = {}
        
    def monitor_resources(self, process_name: str):
        """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ù…Ù†Ø§Ø¨Ø¹ Ø³ÛŒØ³ØªÙ…"""
        process = psutil.Process()
        
        return {
            'cpu_percent': process.cpu_percent(),
            'memory_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'process_name': process_name,
            'timestamp': datetime.now()
        }
    
    def test_feature_extraction_sync(self, sample_size: int = 1000):
        """ØªØ³Øª sync feature extraction"""
        logger.info(f"ğŸ”„ ØªØ³Øª Sync Feature Extraction ({sample_size:,} users)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("sync_feature_extraction")
        
        try:
            db_manager = SQLiteManager(self.db_path)
            extractor = BankingFeatureExtractor(db_manager)
            
            # Ø¯Ø±ÛŒØ§ÙØª sample users
            users_sample = db_manager.execute_query(
                f"SELECT user_id FROM users ORDER BY user_id LIMIT {sample_size}"
            )
            user_ids = users_sample['user_id'].tolist()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ sync
            features_df = extractor.extract_features_batch(user_ids, batch_size=100)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("sync_feature_extraction")
            
            result = {
                'processing_time': processing_time,
                'users_processed': len(features_df),
                'features_extracted': len(features_df.columns) if not features_df.empty else 0,
                'speed_users_per_sec': len(features_df) / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Sync Feature Extraction: {processing_time:.2f}s, {result['speed_users_per_sec']:.1f} users/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± sync feature extraction: {e}")
            return {'error': str(e)}
    
    async def test_feature_extraction_async(self, sample_size: int = 1000):
        """ØªØ³Øª async feature extraction"""
        logger.info(f"ğŸš€ ØªØ³Øª Async Feature Extraction ({sample_size:,} users)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("async_feature_extraction")
        
        try:
            db_manager = SQLiteManager(self.db_path)
            extractor = AsyncBankingFeatureExtractor(db_manager, max_workers=8)
            
            # Ø¯Ø±ÛŒØ§ÙØª sample users
            users_sample = db_manager.execute_query(
                f"SELECT user_id FROM users ORDER BY user_id LIMIT {sample_size}"
            )
            user_ids = users_sample['user_id'].tolist()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ async
            features_df = await extractor.extract_features_batch_async(user_ids, batch_size=100)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("async_feature_extraction")
            
            result = {
                'processing_time': processing_time,
                'users_processed': len(features_df),
                'features_extracted': len(features_df.columns) if not features_df.empty else 0,
                'speed_users_per_sec': len(features_df) / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Async Feature Extraction: {processing_time:.2f}s, {result['speed_users_per_sec']:.1f} users/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± async feature extraction: {e}")
            return {'error': str(e)}
    
    def test_clustering_sync(self, sample_size: int = 5000):
        """ØªØ³Øª sync clustering"""
        logger.info(f"ğŸ”„ ØªØ³Øª Sync Clustering ({sample_size:,} samples)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("sync_clustering")
        
        try:
            db_manager = SQLiteManager(self.db_path)
            analyzer = BankingCustomerClustering(db_manager)
            
            # Ø§Ø¬Ø±Ø§ÛŒ clustering
            results = analyzer.run_complete_clustering_analysis(sample_size=sample_size)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("sync_clustering")
            
            result = {
                'processing_time': processing_time,
                'samples_processed': sample_size,
                'best_method': results.get('best_method', 'Unknown'),
                'best_score': results.get('best_score', 0),
                'speed_samples_per_sec': sample_size / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Sync Clustering: {processing_time:.2f}s, {result['speed_samples_per_sec']:.1f} samples/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± sync clustering: {e}")
            return {'error': str(e)}
    
    async def test_clustering_async(self, sample_size: int = 5000):
        """ØªØ³Øª async clustering"""
        logger.info(f"ğŸš€ ØªØ³Øª Async Clustering ({sample_size:,} samples)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("async_clustering")
        
        try:
            from src.analysis.async_clustering import run_async_clustering_analysis
            
            db_manager = SQLiteManager(self.db_path)
            
            # Ø§Ø¬Ø±Ø§ÛŒ async clustering
            results = await run_async_clustering_analysis(db_manager, sample_size, max_workers=4)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("async_clustering")
            
            result = {
                'processing_time': processing_time,
                'samples_processed': sample_size,
                'best_method': results.get('best_method', 'Unknown'),
                'best_score': results.get('best_score', 0),
                'speed_samples_per_sec': sample_size / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Async Clustering: {processing_time:.2f}s, {result['speed_samples_per_sec']:.1f} samples/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± async clustering: {e}")
            return {'error': str(e)}
    
    def test_anomaly_detection_sync(self, sample_size: int = 3000):
        """ØªØ³Øª sync anomaly detection"""
        logger.info(f"ğŸ”„ ØªØ³Øª Sync Anomaly Detection ({sample_size:,} samples)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("sync_anomaly")
        
        try:
            db_manager = SQLiteManager(self.db_path)
            detector = BankingAnomalyDetector(db_manager)
            
            # Ø§Ø¬Ø±Ø§ÛŒ anomaly detection
            results = detector.run_complete_anomaly_analysis(sample_size=sample_size)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("sync_anomaly")
            
            result = {
                'processing_time': processing_time,
                'samples_processed': sample_size,
                'total_anomalies': results.get('total_anomalies_detected', 0),
                'best_method': results.get('best_method', 'Unknown'),
                'speed_samples_per_sec': sample_size / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Sync Anomaly Detection: {processing_time:.2f}s, {result['speed_samples_per_sec']:.1f} samples/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± sync anomaly detection: {e}")
            return {'error': str(e)}
    
    async def test_anomaly_detection_async(self, sample_size: int = 3000):
        """ØªØ³Øª async anomaly detection"""
        logger.info(f"ğŸš€ ØªØ³Øª Async Anomaly Detection ({sample_size:,} samples)...")
        
        start_time = time.time()
        start_resources = self.monitor_resources("async_anomaly")
        
        try:
            from src.analysis.async_anomaly_detection import run_async_anomaly_analysis
            
            db_manager = SQLiteManager(self.db_path)
            
            # Ø§Ø¬Ø±Ø§ÛŒ async anomaly detection
            results = await run_async_anomaly_analysis(db_manager, sample_size, max_workers=4)
            
            processing_time = time.time() - start_time
            end_resources = self.monitor_resources("async_anomaly")
            
            result = {
                'processing_time': processing_time,
                'samples_processed': sample_size,
                'total_anomalies': results.get('total_anomalies_detected', 0),
                'best_method': results.get('best_method', 'Unknown'),
                'speed_samples_per_sec': sample_size / processing_time if processing_time > 0 else 0,
                'start_resources': start_resources,
                'end_resources': end_resources,
                'memory_peak_mb': end_resources['memory_mb'] - start_resources['memory_mb']
            }
            
            logger.info(f"âœ… Async Anomaly Detection: {processing_time:.2f}s, {result['speed_samples_per_sec']:.1f} samples/sec")
            
            db_manager.close()
            return result
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± async anomaly detection: {e}")
            return {'error': str(e)}
    
    async def run_comprehensive_performance_test(self):
        """ØªØ³Øª Ø¬Ø§Ù…Ø¹ performance"""
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øª Ø¬Ø§Ù…Ø¹ Performance...")
        
        overall_start = time.time()
        
        # ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        test_configs = [
            # (process, sample_size)
            ('feature_extraction', 1000),
            ('clustering', 5000),
            ('anomaly_detection', 3000)
        ]
        
        for process, sample_size in test_configs:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ”¬ ØªØ³Øª {process.upper()} (Sample: {sample_size:,})")
            logger.info(f"{'='*60}")
            
            # ØªØ³Øª sync
            sync_method = getattr(self, f'test_{process}_sync')
            sync_result = sync_method(sample_size)
            
            # ØªØ³Øª async
            async_method = getattr(self, f'test_{process}_async')
            async_result = await async_method(sample_size)
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯
            if 'error' not in sync_result and 'error' not in async_result:
                improvement = sync_result['processing_time'] / async_result['processing_time']
                
                self.results[process] = {
                    'sync': sync_result,
                    'async': async_result,
                    'improvement_factor': improvement,
                    'time_saved_seconds': sync_result['processing_time'] - async_result['processing_time'],
                    'speed_improvement_percent': ((improvement - 1) * 100)
                }
                
                logger.info(f"ğŸ“Š Ø¨Ù‡Ø¨ÙˆØ¯ {process}: {improvement:.1f}x Ø³Ø±ÛŒØ¹â€ŒØªØ±")
                logger.info(f"â±ï¸  ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ø²Ù…Ø§Ù†: {self.results[process]['time_saved_seconds']:.1f} Ø«Ø§Ù†ÛŒÙ‡")
            else:
                self.results[process] = {
                    'sync': sync_result,
                    'async': async_result,
                    'error': True
                }
        
        total_time = time.time() - overall_start
        
        logger.info(f"\nğŸ‰ ØªØ³Øª Ø¬Ø§Ù…Ø¹ Ø¯Ø± {total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        return self.results
    
    def generate_performance_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ performance"""
        logger.info("ğŸ“Š ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ performance...")
        
        try:
            # Ø§ÛŒØ¬Ø§Ø¯ DataFrame
            report_data = []
            
            for process, data in self.results.items():
                if 'error' not in data:
                    report_data.append({
                        'Process': process.replace('_', ' ').title(),
                        'Sync Time (s)': round(data['sync']['processing_time'], 2),
                        'Async Time (s)': round(data['async']['processing_time'], 2),
                        'Improvement (x)': round(data['improvement_factor'], 1),
                        'Time Saved (s)': round(data['time_saved_seconds'], 1),
                        'Speed Improvement (%)': round(data['speed_improvement_percent'], 1),
                        'Sync Memory (MB)': round(data['sync']['memory_peak_mb'], 1),
                        'Async Memory (MB)': round(data['async']['memory_peak_mb'], 1)
                    })
            
            if report_data:
                df = pd.DataFrame(report_data)
                
                # Ù†Ù…Ø§ÛŒØ´ Ø¬Ø¯ÙˆÙ„
                print("\n" + "="*100)
                print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Performance Sync vs Async")
                print("="*100)
                print(df.to_string(index=False))
                print("="*100)
                
                # Ø°Ø®ÛŒØ±Ù‡ CSV
                Path("output/reports").mkdir(parents=True, exist_ok=True)
                df.to_csv("output/reports/performance_comparison.csv", index=False)
                
                # Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ
                total_sync_time = df['Sync Time (s)'].sum()
                total_async_time = df['Async Time (s)'].sum()
                overall_improvement = total_sync_time / total_async_time
                
                print(f"\nğŸ¯ Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ:")
                print(f"   â€¢ Ú©Ù„ Ø²Ù…Ø§Ù† Sync: {total_sync_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
                print(f"   â€¢ Ú©Ù„ Ø²Ù…Ø§Ù† Async: {total_async_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
                print(f"   â€¢ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ: {overall_improvement:.1f}x")
                print(f"   â€¢ ØµØ±ÙÙ‡â€ŒØ¬ÙˆÛŒÛŒ Ú©Ù„: {total_sync_time - total_async_time:.1f} Ø«Ø§Ù†ÛŒÙ‡")
                
                return {
                    'report_df': df,
                    'overall_improvement': overall_improvement,
                    'total_time_saved': total_sync_time - total_async_time
                }
            else:
                logger.warning("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯")
                return None
                
        except Exception as e:
            logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´: {e}")
            return None

async def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    logger.info("Ø´Ø±ÙˆØ¹ ØªØ³Øª Performance Async vs Sync...")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ tester
        tester = AsyncPerformanceTester()
        
        # Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§
        results = await tester.run_comprehensive_performance_test()
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        report = tester.generate_performance_report()
        
        if report:
            print(f"\nâœ… ØªØ³Øª performance Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
            print(f"ğŸ“ Ú¯Ø²Ø§Ø±Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: output/reports/performance_comparison.csv")
        else:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´")
            
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {e}")

if __name__ == "__main__":
    asyncio.run(main()) 
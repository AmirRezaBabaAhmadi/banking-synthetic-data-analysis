#!/usr/bin/env python3
"""
ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù†Ú©ÛŒ Synthetic

Ø§Ø³ØªÙØ§Ø¯Ù‡:
    python main.py --sample       # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (1000 Ú©Ø§Ø±Ø¨Ø±)
    python main.py --full         # ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ (1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±)
    python main.py --analysis     # Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯
    python main.py --all          # Ø§Ù†Ø¬Ø§Ù… Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„
"""

import argparse
import logging
import sys
import os
from datetime import datetime

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src Ø¨Ù‡ Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.data_generation.generators import BankingDataGenerator, NewUserGenerator
from src.database.sqlite_manager import SQLiteManager
from src.utils.config import get_config, setup_directories

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('banking_data_generation.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)

def generate_sample_data():
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ===")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ generator
        db_manager = SQLiteManager("database/banking_sample.db")
        generator = BankingDataGenerator(db_manager)
        
        # ØªÙˆÙ„ÛŒØ¯ 1000 Ú©Ø§Ø±Ø¨Ø± Ù†Ù…ÙˆÙ†Ù‡
        stats = generator.generate_sample_data(num_users=1000)
        
        # ØµØ§Ø¯Ø±Ø§Øª Ú¯Ø²Ø§Ø±Ø´
        generator.export_generation_report("output/reports/sample_generation_report.txt")
        
        # ØµØ§Ø¯Ø±Ø§Øª Ø¢Ù…Ø§Ø± Ù†ÙˆÛŒØ²
        generator.noise_injector.export_noise_analysis("output/reports/sample_noise_analysis.txt")
        
        logger.info("ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {stats['total_users_generated']:,} Ú©Ø§Ø±Ø¨Ø± Ùˆ {stats['total_transactions_generated']:,} ØªØ±Ø§Ú©Ù†Ø´")
        
        return stats
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡: {e}")
        raise
    finally:
        if 'generator' in locals():
            generator.close()

def generate_full_dataset():
    """ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ 1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ ===")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ generator
        db_manager = SQLiteManager()
        generator = BankingDataGenerator(db_manager)
        
        # ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„
        stats = generator.generate_complete_dataset()
        
        # ØµØ§Ø¯Ø±Ø§Øª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        generator.export_generation_report("output/reports/full_generation_report.txt")
        generator.noise_injector.export_noise_analysis("output/reports/full_noise_analysis.txt")
        
        logger.info("ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {stats['total_users_generated']:,} Ú©Ø§Ø±Ø¨Ø± Ùˆ {stats['total_transactions_generated']:,} ØªØ±Ø§Ú©Ù†Ø´")
        
        return stats
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„: {e}")
        raise
    finally:
        if 'generator' in locals():
            generator.close()

def run_analysis():
    """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ===")
    
    try:
        from src.feature_engineering.extractors import BankingFeatureExtractor
        from src.feature_engineering.transformers import FeatureTransformer
        from src.analysis.clustering import BankingCustomerClustering
        from src.analysis.anomaly_detection import BankingAnomalyDetector
        from src.analysis.similarity_search import BankingSimilaritySearch
        from src.utils.visualization import BankingVisualizationUtils
        
        # Ù…Ø¯ÛŒØ± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        db_manager = SQLiteManager()
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡
        users_count = db_manager.execute_query("SELECT COUNT(*) as count FROM users").iloc[0]['count']
        if users_count == 0:
            logger.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
            return
        
        logger.info(f"ÛŒØ§ÙØª Ø´Ø¯: {users_count:,} Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„")
        
        # 1. Feature Engineering
        logger.info("1ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ...")
        feature_extractor = BankingFeatureExtractor(db_manager)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ (Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆÚ†Ú© Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª)
        sample_size = min(10000, users_count)
        all_users = db_manager.execute_query("SELECT user_id FROM users ORDER BY RANDOM()")
        sample_user_ids = all_users['user_id'].head(sample_size).tolist()
        
        features_df = feature_extractor.extract_features_batch(sample_user_ids)
        if not features_df.empty:
            feature_extractor.save_features_to_database(features_df)
            logger.info(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(features_df.columns)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ {len(features_df)} Ú©Ø§Ø±Ø¨Ø±")
        
        # 2. Clustering Analysis
        logger.info("2ï¸âƒ£ ØªØ­Ù„ÛŒÙ„ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ...")
        clustering_analyzer = BankingCustomerClustering(db_manager)
        clustering_results = clustering_analyzer.run_complete_clustering_analysis(sample_size=sample_size)
        logger.info(f"Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ clustering: {clustering_results.get('best_method', 'Ù‡ÛŒÚ†')}")
        
        # 3. Anomaly Detection
        logger.info("3ï¸âƒ£ ØªØ´Ø®ÛŒØµ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ...")
        anomaly_detector = BankingAnomalyDetector(db_manager)
        anomaly_results = anomaly_detector.run_complete_anomaly_analysis(sample_size=sample_size)
        logger.info(f"Ú©Ø´Ù Ø´Ø¯: {anomaly_results.get('total_anomalies_detected', 0)} Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ")
        
        # 4. Similarity Search
        logger.info("4ï¸âƒ£ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ´Ø§Ø¨Ù‡...")
        similarity_searcher = BankingSimilaritySearch(db_manager)
        similarity_results = similarity_searcher.run_complete_similarity_analysis()
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {similarity_results.get('new_test_users', 0)} Ú©Ø§Ø±Ø¨Ø± ØªØ³Øª Ø¬Ø¯ÛŒØ¯")
        
        # 5. Comprehensive Visualization
        logger.info("5ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ±Ø³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹...")
        visualizer = BankingVisualizationUtils()
        
        # Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¬Ø§Ù…Ø¹
        transactions_sample = db_manager.execute_query(
            "SELECT * FROM transactions ORDER BY RANDOM() LIMIT 10000"
        )
        
        dashboard_fig = visualizer.create_comprehensive_dashboard(
            features_df, 
            transactions_sample
        )
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        analysis_summary = {
            'feature_engineering': {
                'features_extracted': len(features_df.columns),
                'users_processed': len(features_df)
            },
            'clustering': {
                'best_method': clustering_results.get('best_method'),
                'best_score': clustering_results.get('best_score'),
                'clusters_found': len(clustering_results.get('cluster_profiles', {}))
            },
            'anomaly_detection': {
                'total_anomalies': anomaly_results.get('total_anomalies_detected', 0),
                'best_method': anomaly_results.get('best_method'),
                'detection_methods': list(anomaly_results.get('methods_results', {}).keys())
            },
            'similarity_search': {
                'new_test_users': similarity_results.get('new_test_users', 0),
                'total_users': similarity_results.get('total_users', 0),
                'avg_similarity': similarity_results.get('search_stats', {}).get('avg_similarity_score', 0)
            }
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
        summary_path = "output/analysis_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            import json
            import numpy as np
            
            # ØªØ¨Ø¯ÛŒÙ„ numpy types Ø¨Ù‡ Python native types
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
            
            analysis_summary_clean = convert_numpy_types(analysis_summary)
            json.dump(analysis_summary_clean, f, indent=2, ensure_ascii=False)
        
        logger.info("ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        logger.info(f"Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {summary_path}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
        print("\n" + "="*60)
        print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„")
        print("="*60)
        print(f"ğŸ”§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {analysis_summary['feature_engineering']['features_extracted']}")
        print(f"ğŸ‘¥ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡: {analysis_summary['feature_engineering']['users_processed']:,}")
        print(f"ğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ clustering: {analysis_summary['clustering']['best_method']}")
        print(f"âš ï¸  Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø´Ùâ€ŒØ´Ø¯Ù‡: {analysis_summary['anomaly_detection']['total_anomalies']:,}")
        print(f"ğŸ†• Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ØªØ³Øª Ø¬Ø¯ÛŒØ¯: {analysis_summary['similarity_search']['new_test_users']}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§: {e}")
        raise

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    parser = argparse.ArgumentParser(description='Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù†Ú©ÛŒ Synthetic')
    
    parser.add_argument('--sample', action='store_true', 
                       help='ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (1000 Ú©Ø§Ø±Ø¨Ø±)')
    parser.add_argument('--full', action='store_true', 
                       help='ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ (1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±)')
    parser.add_argument('--analysis', action='store_true', 
                       help='Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯')
    parser.add_argument('--all', action='store_true', 
                       help='Ø§Ù†Ø¬Ø§Ù… Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„')
    
    args = parser.parse_args()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§
    if not any([args.sample, args.full, args.analysis, args.all]):
        parser.print_help()
        return
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
    logger.info("Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡...")
    setup_directories()
    
    start_time = datetime.now()
    
    try:
        if args.sample or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡...")
            sample_stats = generate_sample_data()
            logger.info("âœ… ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        if args.full or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„...")
            full_stats = generate_full_dataset()
            logger.info("âœ… ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        if args.analysis or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
            run_analysis()
            logger.info("âœ… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"ğŸ‰ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¯Ø± {total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        raise

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù†Ú©ÛŒ Synthetic

Ø§Ø³ØªÙØ§Ø¯Ù‡:
    python main.py --sample       # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (1000 Ú©Ø§Ø±Ø¨Ø±)
    python main.py --full         # ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ (1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±)
    python main.py --analysis     # Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯
    python main.py --all          # Ø§Ù†Ø¬Ø§Ù… Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„
"""

import argparse
import logging
import sys
import os
import asyncio
from datetime import datetime

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± src Ø¨Ù‡ Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.data_generation.generators import BankingDataGenerator, NewUserGenerator
from src.database.sqlite_manager import SQLiteManager
from src.utils.config import get_config, setup_directories

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('banking_data_generation.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)

def generate_sample_data():
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ===")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ generator
        db_manager = SQLiteManager("database/banking_sample.db")
        generator = BankingDataGenerator(db_manager)
        
        # ØªÙˆÙ„ÛŒØ¯ 1000 Ú©Ø§Ø±Ø¨Ø± Ù†Ù…ÙˆÙ†Ù‡
        stats = generator.generate_sample_data(num_users=1000)
        
        # ØµØ§Ø¯Ø±Ø§Øª Ú¯Ø²Ø§Ø±Ø´
        generator.export_generation_report("output/reports/sample_generation_report.txt")
        
        # ØµØ§Ø¯Ø±Ø§Øª Ø¢Ù…Ø§Ø± Ù†ÙˆÛŒØ²
        generator.noise_injector.export_noise_analysis("output/reports/sample_noise_analysis.txt")
        
        logger.info("ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {stats['total_users_generated']:,} Ú©Ø§Ø±Ø¨Ø± Ùˆ {stats['total_transactions_generated']:,} ØªØ±Ø§Ú©Ù†Ø´")
        
        return stats
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡: {e}")
        raise
    finally:
        if 'generator' in locals():
            generator.close()

def generate_full_dataset():
    """ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ 1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ ===")
    
    try:
        # Ø§ÛŒØ¬Ø§Ø¯ generator
        db_manager = SQLiteManager()
        generator = BankingDataGenerator(db_manager)
        
        # ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„
        stats = generator.generate_complete_dataset()
        
        # ØµØ§Ø¯Ø±Ø§Øª Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        generator.export_generation_report("output/reports/full_generation_report.txt")
        generator.noise_injector.export_noise_analysis("output/reports/full_noise_analysis.txt")
        
        logger.info("ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
        logger.info(f"ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {stats['total_users_generated']:,} Ú©Ø§Ø±Ø¨Ø± Ùˆ {stats['total_transactions_generated']:,} ØªØ±Ø§Ú©Ù†Ø´")
        
        return stats
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„: {e}")
        raise
    finally:
        if 'generator' in locals():
            generator.close()

def run_analysis():
    """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ (Ù†Ø³Ø®Ù‡ sync)"""
    return asyncio.run(run_analysis_async())

async def run_analysis_async():
    """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø§ async processing"""
    logger.info("=== Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Async Processing ===")
    
    try:
        from src.feature_engineering.extractors import BankingFeatureExtractor
        from src.feature_engineering.async_extractors import AsyncBankingFeatureExtractor, run_async_feature_extraction
        from src.feature_engineering.transformers import FeatureTransformer
        from src.analysis.clustering import BankingCustomerClustering
        from src.analysis.async_clustering import AsyncBankingCustomerClustering, run_async_clustering_analysis
        from src.analysis.anomaly_detection import BankingAnomalyDetector
        from src.analysis.async_anomaly_detection import AsyncBankingAnomalyDetection, run_async_anomaly_analysis
        from src.analysis.similarity_search import BankingSimilaritySearch
        from src.utils.visualization import BankingVisualizationUtils
        
        # Ù…Ø¯ÛŒØ± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² async manager Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± async context
        from src.database.async_sqlite_manager import AsyncSQLiteManager
        db_manager = AsyncSQLiteManager()
        await db_manager.initialize()
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡
        users_df = await db_manager.execute_query("SELECT COUNT(*) as count FROM users")
        users_count = users_df.get_column('count')[0] if not users_df.is_empty() else 0
        if users_count == 0:
            logger.error("Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø¨ØªØ¯Ø§ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
            return
        
        logger.info(f"ÛŒØ§ÙØª Ø´Ø¯: {users_count:,} Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„")
        
        # 1. Async Feature Engineering
        logger.info("1ï¸âƒ£ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ async...")
        async_feature_extractor = AsyncBankingFeatureExtractor(db_manager, max_workers=8)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ async ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†
        logger.info(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ async ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ {users_count:,} Ú©Ø§Ø±Ø¨Ø±...")
        features_df = await async_feature_extractor.extract_all_user_features_async(
            batch_size=1000, concurrent_batches=3
        )
        
        if not features_df.empty:
            # Ø°Ø®ÛŒØ±Ù‡ async
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, 
                lambda: async_feature_extractor.save_features_to_database(features_df)
            )
            logger.info(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯: {len(features_df.columns)} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ {len(features_df)} Ú©Ø§Ø±Ø¨Ø±")
        
        # 2. Async Clustering Analysis & Anomaly Detection (concurrent)
        logger.info("ğŸš€ Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ clustering Ùˆ anomaly detection...")
        
        clustering_sample_size = min(50000, users_count)
        anomaly_sample_size = min(30000, users_count)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆØ§Ø²ÛŒ clustering Ùˆ anomaly detection
        clustering_task = run_async_clustering_analysis(db_manager, clustering_sample_size, max_workers=4)
        anomaly_task = run_async_anomaly_analysis(db_manager, anomaly_sample_size, max_workers=4)
        
        # Ù…Ù†ØªØ¸Ø± Ù…Ø§Ù†Ø¯Ù† Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„ Ù‡Ø± Ø¯Ùˆ
        clustering_results, anomaly_results = await asyncio.gather(clustering_task, anomaly_task)
        
        logger.info(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ clustering: {clustering_results.get('best_method', 'Ù‡ÛŒÚ†')}")
        logger.info(f"âœ… Ú©Ø´Ù Ø´Ø¯: {anomaly_results.get('total_anomalies_detected', 0)} Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ")
        
        # 3. Async Similarity Search
        logger.info("3ï¸âƒ£ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ´Ø§Ø¨Ù‡ async...")
        similarity_searcher = BankingSimilaritySearch(db_manager)
        
        # Ø§Ø¬Ø±Ø§ÛŒ async similarity search
        loop = asyncio.get_event_loop()
        similarity_results = await loop.run_in_executor(None, 
            similarity_searcher.run_complete_similarity_analysis
        )
        logger.info(f"âœ… ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {similarity_results.get('new_test_users', 0)} Ú©Ø§Ø±Ø¨Ø± ØªØ³Øª Ø¬Ø¯ÛŒØ¯")
        
        # 4. Async Visualization
        logger.info("4ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ ØªØµÙˆÛŒØ±Ø³Ø§Ø²ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø§Ù…Ø¹ async...")
        visualizer = BankingVisualizationUtils()
        
        # Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¬Ø§Ù…Ø¹ Ø¨Ø§ sample Ù…Ù†Ø§Ø³Ø¨ - async
        transactions_task = db_manager.execute_query("SELECT * FROM transactions ORDER BY RANDOM() LIMIT 50000")
        
        transactions_sample = await transactions_task
        
        dashboard_task = loop.run_in_executor(None, lambda: visualizer.create_comprehensive_dashboard(
            features_df.sample(n=min(10000, len(features_df)), random_state=42) if len(features_df) > 10000 else features_df, 
            transactions_sample
        ))
        
        dashboard_fig = await dashboard_task
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        analysis_summary = {
            'feature_engineering': {
                'features_extracted': len(features_df.columns),
                'users_processed': len(features_df)
            },
            'clustering': {
                'best_method': clustering_results.get('best_method'),
                'best_score': clustering_results.get('best_score'),
                'clusters_found': len(clustering_results.get('cluster_profiles', {})),
                'sample_size_used': clustering_sample_size
            },
            'anomaly_detection': {
                'total_anomalies': anomaly_results.get('total_anomalies_detected', 0),
                'best_method': anomaly_results.get('best_method'),
                'detection_methods': list(anomaly_results.get('methods_results', {}).keys()),
                'sample_size_used': anomaly_sample_size
            },
            'similarity_search': {
                'new_test_users': similarity_results.get('new_test_users', 0),
                'total_users': similarity_results.get('total_users', 0),
                'avg_similarity': similarity_results.get('search_stats', {}).get('avg_similarity_score', 0)
            },
            'total_users_in_database': users_count
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡
        summary_path = "output/analysis_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            import json
            import numpy as np
            
            # ØªØ¨Ø¯ÛŒÙ„ numpy types Ø¨Ù‡ Python native types
            def convert_numpy_types(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                else:
                    return obj
            
            analysis_summary_clean = convert_numpy_types(analysis_summary)
            json.dump(analysis_summary_clean, f, indent=2, ensure_ascii=False)
        
        logger.info("ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        logger.info(f"Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {summary_path}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡
        print("\n" + "="*60)
        print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„")
        print("="*60)
        print(f"ğŸ“Š Ú©Ù„ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {analysis_summary['total_users_in_database']:,}")
        print(f"ğŸ”§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {analysis_summary['feature_engineering']['features_extracted']}")
        print(f"ğŸ‘¥ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {analysis_summary['feature_engineering']['users_processed']:,}")
        print(f"ğŸ¯ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ clustering: {analysis_summary['clustering']['best_method']} (sample: {analysis_summary['clustering']['sample_size_used']:,})")
        print(f"âš ï¸  Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø´Ùâ€ŒØ´Ø¯Ù‡: {analysis_summary['anomaly_detection']['total_anomalies']:,} (sample: {analysis_summary['anomaly_detection']['sample_size_used']:,})")
        print(f"ğŸ†• Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ØªØ³Øª Ø¬Ø¯ÛŒØ¯: {analysis_summary['similarity_search']['new_test_users']}")
        print("="*60)
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§: {e}")
        raise

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    parser = argparse.ArgumentParser(description='Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù†Ú©ÛŒ Synthetic')
    
    parser.add_argument('--sample', action='store_true', 
                       help='ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ (1000 Ú©Ø§Ø±Ø¨Ø±)')
    parser.add_argument('--full', action='store_true', 
                       help='ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ (1 Ù…ÛŒÙ„ÛŒÙˆÙ† Ú©Ø§Ø±Ø¨Ø±)')
    parser.add_argument('--analysis', action='store_true', 
                       help='Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ Ù…ÙˆØ¬ÙˆØ¯')
    parser.add_argument('--all', action='store_true', 
                       help='Ø§Ù†Ø¬Ø§Ù… Ù‡Ù…Ù‡ Ù…Ø±Ø§Ø­Ù„')
    
    args = parser.parse_args()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§
    if not any([args.sample, args.full, args.analysis, args.all]):
        parser.print_help()
        return
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
    logger.info("Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø±ÙˆÚ˜Ù‡...")
    setup_directories()
    
    start_time = datetime.now()
    
    try:
        if args.sample or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡...")
            sample_stats = generate_sample_data()
            logger.info("âœ… ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÙˆÙ†Ù‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        if args.full or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„...")
            full_stats = generate_full_dataset()
            logger.info("âœ… ØªÙˆÙ„ÛŒØ¯ dataset Ú©Ø§Ù…Ù„ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        if args.analysis or args.all:
            logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
            run_analysis()
            logger.info("âœ… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"ğŸ‰ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ø¯Ø± {total_time:.2f} Ø«Ø§Ù†ÛŒÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯")
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  Ø¹Ù…Ù„ÛŒØ§Øª ØªÙˆØ³Ø· Ú©Ø§Ø±Ø¨Ø± Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")
        raise

if __name__ == "__main__":
    main() 